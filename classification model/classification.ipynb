{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0ulowUkK115"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'base (Python 3.12.3)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ===================== Parameters =====================\n",
        "IMG_SIZE = 512\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "\n",
        "# ===================== Load dataset =====================\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "\n",
        "X, y = [], []\n",
        "for index, row in df.iterrows():\n",
        "    img_path = f\"./images/{row['image']}\"\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        continue\n",
        "    old_size = img.shape[:2]\n",
        "    ratio = float(IMG_SIZE) / max(old_size)\n",
        "    new_size = tuple([int(x * ratio) for x in old_size])\n",
        "    img_resized = cv2.resize(img, (new_size[1], new_size[0]))\n",
        "    delta_w = IMG_SIZE - new_size[1]\n",
        "    delta_h = IMG_SIZE - new_size[0]\n",
        "    top, bottom = delta_h // 2, delta_h - delta_h // 2\n",
        "    left, right = delta_w // 2, delta_w - delta_w // 2\n",
        "    padded_img = cv2.copyMakeBorder(img_resized, top, bottom, left, right,\n",
        "                                    cv2.BORDER_CONSTANT, value=0)\n",
        "    X.append(padded_img)\n",
        "    y.append(row['tumor'])\n",
        "\n",
        "X = np.array(X, dtype='float32')\n",
        "X = preprocess_input(np.repeat(np.expand_dims(X, -1), 3, axis=-1))\n",
        "y = np.array(y)\n",
        "\n",
        "# Train-validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# ===================== Data Augmentation =====================\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomContrast(0.1),\n",
        "    layers.RandomBrightness(0.1)\n",
        "])\n",
        "\n",
        "# ===================== Class weights =====================\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# ===================== Model =====================\n",
        "base_model = EfficientNetB3(\n",
        "    include_top=False,\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# Unfreeze the last convolutional block (~20 layers)\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-40]:\n",
        "    layer.trainable = False\n",
        "\n",
        "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "# ===================== Compile =====================\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')]\n",
        ")\n",
        "\n",
        "# ===================== Callbacks =====================\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, min_lr=1e-7, verbose=1),\n",
        "    ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "# ===================== Train =====================\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# ===================== Save Model =====================\n",
        "model.save(\"classification.keras\")\n",
        "\n",
        "# ===================== Evaluate =====================\n",
        "y_pred = (model.predict(X_val) > 0.5).astype(int)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred, digits=4))\n",
        "\n",
        "# ===================== Plot Metrics =====================\n",
        "def plot_metric(history, metric):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(history.history[metric], label=f'Train {metric}')\n",
        "    plt.plot(history.history[f'val_{metric}'], label=f'Val {metric}')\n",
        "    plt.title(f'{metric.capitalize()} over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_metric(history, 'accuracy')\n",
        "plot_metric(history, 'loss')\n",
        "plot_metric(history, 'precision')\n",
        "plot_metric(history, 'recall')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
